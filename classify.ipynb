{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7955897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'id', 'width', 'height', 'license', 'flickr_url', 'coco_url', 'date_captured', 'gps', 'city_name', 'scene_description', 'video_info.frame_id', 'video_info.seq_id', 'video_info.vid_id', 'scene_level_tags.daytime', 'scene_level_tags.scene_environment', 'scene_level_tags.travel_alteration', 'scene_level_tags.weather', 'label'],\n",
      "        num_rows: 6251\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'id', 'width', 'height', 'license', 'flickr_url', 'coco_url', 'date_captured', 'gps', 'city_name', 'scene_description', 'video_info.frame_id', 'video_info.seq_id', 'video_info.vid_id', 'scene_level_tags.daytime', 'scene_level_tags.scene_environment', 'scene_level_tags.travel_alteration', 'scene_level_tags.weather', 'label'],\n",
      "        num_rows: 2298\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# T·∫£i b·ªô d·ªØ li·ªáu t·ª´ Hugging Face Hub\n",
    "dataset = load_dataset(\"natix-network-org/roadwork\")\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856cf114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\n# Duy·ªát v√† l·∫•y m·ªói l·ªõp 1 ·∫£nh ƒë·∫ßu ti√™n\\nsample_label_0 = next(sample for sample in train_ds if sample[\"label\"] == 0)\\nsample_label_1 = next(sample for sample in train_ds if sample[\"label\"] == 1)\\n\\n# Hi·ªÉn th·ªã ·∫£nh c√≥ label = 0\\nprint(\"·∫¢nh m·∫´u thu·ªôc l·ªõp 0:\")\\nplt.imshow(sample_label_0[\"image\"])\\nplt.axis(\"off\")\\nplt.title(\"Label 0\")\\nplt.show()\\n\\n# Hi·ªÉn th·ªã ·∫£nh c√≥ label = 1\\nprint(\"·∫¢nh m·∫´u thu·ªôc l·ªõp 1:\")\\nplt.imshow(sample_label_1[\"image\"])\\nplt.axis(\"off\")\\nplt.title(\"Label 1\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Duy·ªát v√† l·∫•y m·ªói l·ªõp 1 ·∫£nh ƒë·∫ßu ti√™n\n",
    "sample_label_0 = next(sample for sample in train_ds if sample[\"label\"] == 0)\n",
    "sample_label_1 = next(sample for sample in train_ds if sample[\"label\"] == 1)\n",
    "\n",
    "# Hi·ªÉn th·ªã ·∫£nh c√≥ label = 0\n",
    "print(\"·∫¢nh m·∫´u thu·ªôc l·ªõp 0:\")\n",
    "plt.imshow(sample_label_0[\"image\"])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Label 0\")\n",
    "plt.show()\n",
    "\n",
    "# Hi·ªÉn th·ªã ·∫£nh c√≥ label = 1\n",
    "print(\"·∫¢nh m·∫´u thu·ªôc l·ªõp 1:\")\n",
    "plt.imshow(sample_label_1[\"image\"])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Label 1\")\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c55b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ph√¢n ph·ªëi nh√£n ban ƒë·∫ßu: Counter({1: 5031, 0: 1220})\n",
      "‚úÖ C·∫ßn t·∫°o th√™m 3811 ·∫£nh cho l·ªõp 0\n",
      "‚úÖ ƒê√£ t·∫°o 1220 ·∫£nh augment.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# üìä ƒê·∫øm nh√£n\n",
    "train_labels = [ex[\"label\"] for ex in train_ds]\n",
    "train_counts = Counter(train_labels)\n",
    "print(\"üìä Ph√¢n ph·ªëi nh√£n ban ƒë·∫ßu:\", train_counts)\n",
    "\n",
    "# üîç Th√¥ng tin l·ªõp thi·ªÉu s·ªë\n",
    "minority_label = min(train_counts, key=train_counts.get)\n",
    "majority_count = max(train_counts.values())\n",
    "minority_count = train_counts[minority_label]\n",
    "num_to_generate = majority_count - minority_count\n",
    "print(f\"‚úÖ C·∫ßn t·∫°o th√™m {num_to_generate} ·∫£nh cho l·ªõp {minority_label}\")\n",
    "\n",
    "# üì¶ T·∫°o transform augment\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "])\n",
    "\n",
    "# üåÄ T·∫°o ·∫£nh augment t·ª´ng b∆∞·ªõc (tr√°nh gi·ªØ t·∫•t c·∫£ ·∫£nh g·ªëc trong RAM)\n",
    "augmented_data = []\n",
    "generated = 0\n",
    "\n",
    "for ex in train_ds:\n",
    "    if ex[\"label\"] == minority_label:\n",
    "        img = ex[\"image\"]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        aug_img = augment_transform(img)\n",
    "        augmented_data.append({\"image\": aug_img, \"label\": minority_label})\n",
    "        generated += 1\n",
    "\n",
    "        if generated >= num_to_generate:\n",
    "            break\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫°o {len(augmented_data)} ·∫£nh augment.\")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# T·∫°o Dataset t·ª´ augmented_data\n",
    "aug_ds = Dataset.from_list(augmented_data)\n",
    "\n",
    "# N·ªëi 2 dataset m√† kh√¥ng c·∫ßn load v√†o RAM\n",
    "train_ds = concatenate_datasets([train_ds, aug_ds])\n",
    "\n",
    "# ‚úÖ ƒê√£ t·∫°o {len(augmented_data)} ·∫£nh augment.\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# üîÅ G·ªôp l·∫°i d√πng concatenate_datasets (tr√°nh MemoryError)\n",
    "aug_ds = Dataset.from_list(augmented_data)\n",
    "train_ds = concatenate_datasets([train_ds, aug_ds])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc2f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒë·∫∑t t√™n cho c√°c l·ªõp\n",
    "label_names = {\n",
    "    0: \"No Roadwork\",\n",
    "    1: \"Roadwork\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10877eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8691/8691 [10:27<00:00, 13.86 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from torchvision import transforms\n",
    "\n",
    "# D√πng processor t∆∞∆°ng ·ª©ng v·ªõi m√¥ h√¨nh ViT\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "\n",
    "# H√†m ti·ªÅn x·ª≠ l√Ω to√†n b·ªô batch\n",
    "def preprocess(batch):\n",
    "    images = [img.resize((224, 224)) for img in batch[\"image\"]]\n",
    "    encoding = processor(images, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"pixel_values\": encoding[\"pixel_values\"],\n",
    "        \"labels\": batch[\"label\"]\n",
    "    }\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra v·ªõi batch_size nh·ªè ƒë·ªÉ tr√°nh MemoryError\n",
    "train_ds = train_ds.map(preprocess, batched=True, batch_size=16, remove_columns=[\"image\"])\n",
    "test_ds = test_ds.map(preprocess, batched=True, batch_size=16, remove_columns=[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3c3373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfrom collections import Counter\\nimport torch\\n\\n# üõ† Dummy transform kh√¥ng l√†m g√¨ c·∫£\\ndef no_transform(example):\\n    return example\\n\\n# üîÅ G√°n t·∫°m transform \"tr·ªëng\"\\ntrain_ds.set_transform(no_transform)\\ntest_ds.set_transform(no_transform)\\n\\n# ‚úÖ L·∫•y nh√£n an to√†n\\ntrain_labels = [example[\"label\"] for example in train_ds]\\ntest_labels = [example[\"label\"] for example in test_ds]\\n\\n# ‚úÖ G√°n l·∫°i transform g·ªëc\\ntrain_ds.set_transform(transform)\\ntest_ds.set_transform(transform)\\n\\n# üìä ƒê·∫øm s·ªë l∆∞·ª£ng l·ªõp\\ntrain_counts = Counter(train_labels)\\ntest_counts = Counter(test_labels)\\n\\nprint(\"üìä Ph√¢n ph·ªëi nh√£n train:\", train_counts)\\nprint(\"üìä Ph√¢n ph·ªëi nh√£n test:\", test_counts)\\n\\n# ‚öñÔ∏è T√≠nh class weights\\ntotal = sum(train_counts.values())\\nclass_weights = [total / train_counts[i] for i in sorted(train_counts.keys())]\\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\\n\\n# ‚öôÔ∏è D√πng CPU\\ndevice = torch.device(\"cpu\")\\nclass_weights = class_weights.to(device)\\n\\nprint(\"‚úÖ class_weights =\", class_weights)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# üõ† Dummy transform kh√¥ng l√†m g√¨ c·∫£\n",
    "def no_transform(example):\n",
    "    return example\n",
    "\n",
    "# üîÅ G√°n t·∫°m transform \"tr·ªëng\"\n",
    "train_ds.set_transform(no_transform)\n",
    "test_ds.set_transform(no_transform)\n",
    "\n",
    "# ‚úÖ L·∫•y nh√£n an to√†n\n",
    "train_labels = [example[\"label\"] for example in train_ds]\n",
    "test_labels = [example[\"label\"] for example in test_ds]\n",
    "\n",
    "# ‚úÖ G√°n l·∫°i transform g·ªëc\n",
    "train_ds.set_transform(transform)\n",
    "test_ds.set_transform(transform)\n",
    "\n",
    "# üìä ƒê·∫øm s·ªë l∆∞·ª£ng l·ªõp\n",
    "train_counts = Counter(train_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "print(\"üìä Ph√¢n ph·ªëi nh√£n train:\", train_counts)\n",
    "print(\"üìä Ph√¢n ph·ªëi nh√£n test:\", test_counts)\n",
    "\n",
    "# ‚öñÔ∏è T√≠nh class weights\n",
    "total = sum(train_counts.values())\n",
    "class_weights = [total / train_counts[i] for i in sorted(train_counts.keys())]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# ‚öôÔ∏è D√πng CPU\n",
    "device = torch.device(\"cpu\")\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"‚úÖ class_weights =\", class_weights)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd8a37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"No Roadwork\", 1: \"Roadwork\"},\n",
    "    label2id={\"No Roadwork\": 0, \"Roadwork\": 1},\n",
    "    ignore_mismatched_sizes=True  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fb8ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from transformers import AutoModelForImageClassification\\n\\nmodel = AutoModelForImageClassification.from_pretrained(\\n    \"google/vit-base-patch16-224\",\\n    num_labels=2,  # V√¨ c√≥ 2 l·ªõp\\n    id2label=label_names,  # ƒê·∫∑t l·∫°i t√™n l·ªõp cho ƒë·∫πp\\n    label2id={v: k for k, v in label_names.items()},\\n    ignore_mismatched_sizes=True  # Cho ph√©p thay ƒë·ªïi output layer\\n)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=2,  # V√¨ c√≥ 2 l·ªõp\n",
    "    id2label=label_names,  # ƒê·∫∑t l·∫°i t√™n l·ªõp cho ƒë·∫πp\n",
    "    label2id={v: k for k, v in label_names.items()},\n",
    "    ignore_mismatched_sizes=True  # Cho ph√©p thay ƒë·ªïi output layer\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713e29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",  # ƒê√∫ng v·ªõi transformers >= 4.0.0\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf5aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9fff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3001' max='3261' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3001/3261 2:26:04 < 12:39, 0.34 it/s, Epoch 2.76/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.127344</td>\n",
       "      <td>0.965187</td>\n",
       "      <td>0.962241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.152190</td>\n",
       "      <td>0.965622</td>\n",
       "      <td>0.962760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.121565</td>\n",
       "      <td>0.969974</td>\n",
       "      <td>0.969772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.140679</td>\n",
       "      <td>0.966057</td>\n",
       "      <td>0.965662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.147602</td>\n",
       "      <td>0.970844</td>\n",
       "      <td>0.969527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/288 02:31 < 04:47, 0.65 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    for feature in features:\n",
    "        feature.pop(\"id\", None)  # lo·∫°i b·ªè id\n",
    "    return default_data_collator(features)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "# ƒê·∫£m b·∫£o ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c·∫ßn thi·∫øt (pixel_values, labels)\n",
    "training_args.remove_unused_columns = True\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=custom_data_collator  # <<< s·ª≠a ·ªü ƒë√¢y\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p test (ho·∫∑c validation)\n",
    "predictions = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# T√≠nh confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# V·∫Ω confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c372968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "# Load m√¥ h√¨nh v√† processor\n",
    "# N·∫øu ƒë√£ fine-tune xong r·ªìi th√¨ d√πng d√≤ng n√†y ƒë·ªÉ load l·∫°i:\n",
    "# model = ViTForImageClassification.from_pretrained(\"output_model\")\n",
    "# processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Thi·∫øt b·ªã t√≠nh to√°n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ·∫¢nh ng∆∞·ªùi d√πng\n",
    "image_path = r\"C:\\Users\\admin\\Documents\\tlh\\ielts\\task 1\\test1.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image.show()\n",
    "\n",
    "# Ti·ªÅn x·ª≠ l√Ω v√† ƒë∆∞a l√™n ƒë√∫ng thi·∫øt b·ªã\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "label = model.config.id2label[predicted_class]\n",
    "print(f\"·∫¢nh d·ª± ƒëo√°n thu·ªôc l·ªõp: {predicted_class} - {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
